#!/bin/bash

echo "ðŸŒ™ Setting up Luna's Ollama Server..."

# Update system
sudo apt update && sudo apt upgrade -y

# Install Ollama
echo "ðŸ“¦ Installing Ollama..."
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service in background
echo "ðŸš€ Starting Ollama service..."
nohup ollama serve > ollama.log 2>&1 &

# Wait for Ollama to start
echo "â³ Waiting for Ollama to initialize..."
sleep 10

# Download a lightweight model
echo "ðŸ¤– Downloading Phi-3 Mini model (lightweight but capable)..."
ollama pull phi3:mini

# Install Python dependencies for Luna
echo "ðŸ Installing Python dependencies..."
pip install -r requirements.txt

# Create a simple API test script
cat > test_ollama.py << 'EOF'
import requests
import json

def test_ollama():
    try:
        response = requests.post('http://localhost:11434/api/generate', 
            json={
                "model": "phi3:mini",
                "prompt": "Hello! I'm Luna, a cheerful moon spirit. How are you today?",
                "stream": False
            })
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Ollama is working!")
            print("ðŸŒ™ Luna says:", result['response'])
            return True
        else:
            print("âŒ Ollama error:", response.status_code)
            return False
    except Exception as e:
        print("âŒ Connection error:", e)
        return False

if __name__ == "__main__":
    test_ollama()
EOF

echo "âœ… Setup complete!"
echo ""
echo "ðŸŒ™ Luna's Ollama server is ready!"
echo "ðŸ“‹ Next steps:"
echo "   1. Test Ollama: python test_ollama.py"
echo "   2. Check logs: tail -f ollama.log"
echo "   3. API endpoint: http://localhost:11434"
echo ""
echo "ðŸŽ¯ Available models:"
ollama list